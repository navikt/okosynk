apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: okosynk-ur
  namespace: okonomi
  labels:
    team: okonomi
spec:
  groups:
    - name: okosynk-ur-alerts
      rules:
        - alert: okosynkur naisjob mislykket
          expr: kube_job_failed{job_name=~"^okosynkur.*", namespace="okonomi"} > 0
          for: 2m
          annotations:
            action: "Gjør en action"
            summary: "okosynk-ur feiler. Sjekk hvorfor"
          labels:
            namespace: okonomi
            severity: danger
            ktor_job_example_type: okosynk-ur
            alert_type: custom

---

apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: okosynk-ur-slack
  namespace: okonomi
  labels:
    alertmanagerConfig: okosynk-ur-slack
spec:
  receivers:
    - name: okosynk-ur-receiver
      slackConfigs:
        - apiURL:
            key: apiUrl
            name: slack-webhook
          channel: '#team-mob-alerts-dev'
          iconEmoji: ':alert:'
          username: 'Alert dev-gcp'
          sendResolved: true
          title: |-
            [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
          text: >-
            {{ range .Alerts }}
            {{ if or .Annotations.summary .Annotations.message }}
            {{ or .Annotations.summary .Annotations.message }}
            {{ println " " }}
            {{- end }}
            {{- if .Annotations.action }}
            • *action*: {{ .Annotations.action }} {{ println " " }}
            {{- end }}
            {{ end }}
  route:
    groupBy:
      - alertname
    matchers:
      - name: "okosynk_ur_type"
        matchType: "="
        value: "okosynk-ur"
    groupInterval: 10s
    groupWait: 5s
    receiver: okosynk-ur-receiver
    repeatInterval: 2m

---

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: okosynk-alerts
  namespace: okonomi
  labels:
    team: okonomi
spec:
  groups:
    - name: okosynk_general_check_logs_preprod_ur
      rules:
        - alert:
            slack:
              channel: '#team-mob-alerts-dev'
              prependText: 'Okosynk UR issued a warning: '
            email_configs:
              - to: lars.hartvigsen@nav.no
              - to: steinar.hansen@nav.no
            expr: 'sum(rate(okosynk_ur_batch_alert{namespace="okosynk",job="kubernetes-pods"}[24h])) > 0'
            for: 5m
            action: Se på loggene (i Kibana) etter evt. feil og advarsler fra okosynk relatert til UR i preprod
            description: 'Relates to: Okosynk UR: It is as expected that the logs indicate that no further action needs be taken. The potential problems can have been automatically solved by e.g. retries. However, the opposite may also be the case. This alert is based on a counter that counts up for each time an OS batch fails, so it may be anything above 0. 0 indicates, of course, no errors.'
            documentation: okosynk readme.md
            sla: N/A
            severity: danger
