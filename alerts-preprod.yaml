apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: okosynk
  namespace: okonomi
  labels:
    team: okonomi
spec:
  groups:
    - name: okosynk-alerts
      rules:
        - alert: okosynk_general_check_logs_preprod_os
          expr: 'sum(rate(okosynk_os_batch_alert{namespace="okosynk",job="kubernetes-pods"}[24h])) > 0'
          for: 5m
          annotations:
            action: Se på loggene (i Kibana) etter evt. feil og advarsler fra okosynk relatert til OS i preprod
            summary: 'Relates to: Okosynk OS: It is as expected that the logs indicate that no further action needs be taken. The potential problems can have been automatically solved by e.g. retries. However, the opposite may also be the case. This alert is based on a counter that counts up for each time an OS batch fails, so it may be anything above 0. 0 indicates, of course, no errors.'
          labels:
            namespace: okonomi
            severity: danger
            alert_type: custom
---

apiVersion: monitoring.coreos.com/v1
kind: AlertmanagerConfig
metadata:
  name: okosynk-alerts
  namespace: okonomi
  labels:
    alertmanagerConfig: okosynk-slack
spec:
  receivers:
    - name: okosynk_mange_oppgaver_dev_ur
      slackConfigs:
        -apiURL:
          key: apiUrl
          name: slack-webhook
        channel: '#team-mob-alerts-dev'
        iconEmoji: ':alert:'
        username: 'Alert dev-fss'
        title: |-
          Dette er en testkjøring
        text: >-
          Dette er en test på å sende feilmelding til Slack
  route:
    groupBy:
      - alertName
    matchers:
      - name: "okosynk_type"
        matchType: "="
        value: "okosynk"
    groupInterval: 10s
    groupWait: 5s
    receiver: okosynk-receiver
    repeatInterval: 2m
