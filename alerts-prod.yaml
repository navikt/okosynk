apiVersion: nais.io/v1alpha1
kind: Alert
metadata:
  name: okosynk-alerts
  labels:
    team: oppgavehandtering
spec:
  receivers: # receivers for all alerts below
    slack:
      channel: '#oh-prod-varsling'
      prependText: 'Parts of Okosynk may have failed: '
      email_configs:
        - to: oyvind.roth@nav.no
  alerts:
    - alert: okosynk_general_check_logs_prod_os
      expr: 'sum(rate(okosynk_os_batch_alert{namespace="okosynk",job="kubernetes-pods"}[24h])) > 0'
      for: 5m
      action: Se på loggene (i Kibana) etter evt. feil og advarsler fra okosynk relatert til OS i prod
      description: 'Relates to: Okosynk OS: It is as expected that the logs indicate that no further action needs be taken. The potential problems can have been automatically solved by e.g. retries. However, the opposite may also be the case. This alert is based on a counter that counts up for each time an OS batch fails, so it may be anything above 0. 0 indicates, of course, no errors.'
      documentation: okosynk readme.md
      sla: N/A
      severity: danger
    - alert: okosynk_general_check_logs_prod_ur
      expr: 'sum(rate(okosynk_ur_batch_alert{namespace="okosynk",job="kubernetes-pods"}[24h])) > 0'
      for: 5m
      action: Se på loggene (i Kibana) etter evt. feil og advarsler fra okosynk relatert til UR i prod
      description: 'Relates to: Okosynk UR: It is as expected that the logs indicate that no further action needs be taken. The potential problems can have been automatically solved by e.g. retries. However, the opposite may also be the case. This alert is based on a counter that counts up for each time an OS batch fails, so it may be anything above 0. 0 indicates, of course, no errors.'
      documentation: okosynk readme.md
      sla: N/A
      severity: danger